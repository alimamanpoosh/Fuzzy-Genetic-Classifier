{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and pre process the data set\n",
    "def process_data(sms_data_str):\n",
    "    \"\"\"\n",
    "    convert `sms_data_str` into a pandas dataframe\n",
    "    \"\"\"\n",
    "    data_arr = []\n",
    "\n",
    "    data_records = sms_data_str.split('\\n')[:-1]\n",
    "    for data in data_records:\n",
    "        label = None\n",
    "        sample = None\n",
    "        data_type = data[:3]\n",
    "        if data_type == 'ham':\n",
    "            label = 'legitimate'\n",
    "            sample = data[4:] \n",
    "        elif data_type == 'spa':\n",
    "            label = 'spam'\n",
    "            sample = data[5:] \n",
    "        else:\n",
    "            label = 'N/A'\n",
    "            \n",
    "        data_arr.append([label, sample])\n",
    "        \n",
    "    data_arr = np.array(data_arr)\n",
    "    data_label = data_arr[:, 0]\n",
    "    data_records = data_arr[:, 1]\n",
    "    \n",
    "    return data_records, data_label\n",
    "# extract feature from SMS\n",
    "def tfidf_vectorizer(records):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        token_pattern=r'\\b[A-Za-z]+\\b', \n",
    "        norm=None\n",
    "    )\n",
    "    \n",
    "    records_transformed = vectorizer.fit_transform(records)\n",
    "\n",
    "    return records_transformed.toarray(), vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "# decrece the dimension\n",
    "def feature_extraction(X, n_components=5):\n",
    "    reduction_pca = PCA(\n",
    "        n_components=n_components,\n",
    "        whiten=False\n",
    "    )\n",
    "    data_reduced = reduction_pca.fit_transform(X)\n",
    "    return data_reduced\n",
    "\n",
    "# select the feature \n",
    "def feature_selection(df_records, labels, n_components=5):\n",
    "    feature_selection_model = SelectKBest(mutual_info_classif, k=n_components) \n",
    "    ## make a selection over the best features\n",
    "    selected_record_features = feature_selection_model.fit_transform(df_records, labels)\n",
    "    \n",
    "    return selected_record_features, feature_selection_model.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data \n",
    "sms_data_str = None\n",
    "with open('SMSSpamCollection') as file:\n",
    "    sms_data_str = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract feature \n",
    "records, labels = process_data(sms_data_str)\n",
    "records_vectorized, feature_names = tfidf_vectorizer(records)\n",
    "\n",
    "## one hot encoding labels (convert label to 0 or 1)\n",
    "labels = np.array([0 if y == 'legitimate' else 1 for y in labels] )\n",
    "\n",
    "## reducing dimension\n",
    "records_dim_reduced = feature_extraction(records_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.85632519,  0.28319004, -1.18534515,  0.83016254,  0.7252797 ],\n",
       "       [-2.78398453,  0.52105391, -1.74256448,  0.50329886, -0.73155797],\n",
       "       [ 0.48303433, -0.03795902,  2.00931372, -6.52083596,  0.96973512],\n",
       "       [-1.83556457,  1.1391571 , -3.93225827, -0.18171667, -1.97877552],\n",
       "       [ 0.27686514, -0.77516961,  0.11315671,  1.31308522, -0.80324805]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature extraxtion\n",
    "records_dim_reduced[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "for i in records_dim_reduced:\n",
    "    info.append([np.std(i), np.mean(i)])\n",
    "info = pd.DataFrame(info, columns=[\"std\", \"mean\"])\n",
    "\n",
    "info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isosceles_triangular_fuzzy(x, m, s):\n",
    "    return np.max(np.min((x - m) / s, (m - x) / s), 0)\n",
    "def trapezoids_of_Malzawie_fuzzy(x, m, s):\n",
    "    return np.max(np.min((x - m) / s, 1), 0)\n",
    "def gaussian_fuzzy(x, m ,s):\n",
    "    return np.exp(-0.5 * ((x - m) / s) ** 2)\n",
    "def sigmoid_fuzzy(x, m ,s):\n",
    "    return 1 / (1 + np.exp((x - m) / s))\n",
    "# def Fc(R,numberOfrule):\n",
    "    \n",
    "    \n",
    "# def Fneg(Ruls,numberOfrule):\n",
    "#     for i in range(numberOfrule):\n",
    "#         pass\n",
    "\n",
    "def gR(xP, muA):\n",
    "    result = 1\n",
    "    for i in range(len(xP)):\n",
    "        result *= muA[i] * xP[i]\n",
    "    return result\n",
    "\n",
    "def gC(LOFL_xP, LOFL_muA, numberOfrule):  #  nothing\n",
    "    result = 0\n",
    "    for i in range(len(LOFL_xP)):\n",
    "        result += gR(LOFL_xP[i], LOFL_muA[i])\n",
    "    return result\n",
    "\n",
    "\n",
    "def Fc(LOFL_xP, LOFL_muA, numberOfrule):  #\n",
    "    result = 0\n",
    "    for i in range(len(LOFL_xP)):\n",
    "        result += gR(LOFL_xP[i], LOFL_muA[i])\n",
    "    return result\n",
    "\n",
    "def Fneg(LOFL_xP, LOFL_xP, number_another_rule):   # r = number_another_rule\n",
    "    return (1/r) * (Fc(LOFL_xP, LOFL_muA, number_another_rule))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_vectorized = pd.DataFrame(records_vectorized, columns=feature_names)\n",
    "\n",
    "# select the important feature ( data set have many feture , but this code select the some feture in each run)\n",
    "records_selection, feature_name_selection = feature_selection(records_vectorized,labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for better visualization\n",
    "# feature selection\n",
    "pd.DataFrame(records_selection, columns=feature_name_selection).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: build a fuzzy rule-based model for (records, label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b955d226d9aa9f90a51fa3f36ce22332332eca7b3eabee7dfc09f1063aca4ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
